{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f226d6",
   "metadata": {},
   "source": [
    "# Assignment 3: Scalable Quantum Tomography Pipelines\n",
    "This week we push our tomography setup so it can handle many qubits, save trained helpers, and check how well everything scales. Reuse the setup and datasets from earlier weeks. Keep the runs easy to repeat and measure speed properly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bebc87",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a538b19b",
   "metadata": {},
   "source": [
    "## Task 1 · Serialization basics\n",
    "Write down how you will store tomography outputs (model weights, optimiser state, metadata) with pickle. Mention when you would choose another format like HDF5.\n",
    "\n",
    "**What to do**\n",
    "- Add a short note in your report about the save strategy.\n",
    "- Keep checkpoints inside `models/` and name them `model_<track>_<nqubits>.pkl`.\n",
    "- Show save and load in the next cell and keep that helper code ready for later runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization helpers (implement with pickle)\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    \"\"\"TODO: Serialize `obj` to `path` using pickle.\"\"\"\n",
    "    raise NotImplementedError(\"Implement serialization using pickle.dump.\")\n",
    "\n",
    "def load_pickle(path):\n",
    "    \"\"\"TODO: Deserialize an object from `path`.\"\"\"\n",
    "    raise NotImplementedError(\"Implement deserialization using pickle.load.\")\n",
    "\n",
    "def demonstrate_serialization_roundtrip():\n",
    "    \"\"\"TODO: Create a quick round-trip save/load test and return the restored object.\"\"\"\n",
    "    raise NotImplementedError(\"Add a demonstration that exercises save_pickle and load_pickle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cba1602",
   "metadata": {},
   "source": [
    "**Serialization Strategy**\n",
    "\n",
    "For this assignment, I will store tomography outputs by packaging them into a single Python dictionary. This dictionary will contain:\n",
    "\n",
    "1. **Model Weights:** The trainable parameters (e.g., the complex vector or neural network weights).\n",
    "2. **Optimizer State:** To allow resuming training (e.g., momentum/variance buffers).\n",
    "3. **Metadata:** Configuration details such as `n_qubits`, `n_layers`, and the random seed.\n",
    "\n",
    "I will use the `pickle` module to serialize this dictionary into a binary `.pkl` file.\n",
    "\n",
    "**When to choose HDF5?**\n",
    "\n",
    "While `pickle` is convenient for Python-only prototypes, I would choose **HDF5** if:\n",
    "\n",
    "* **Scale:** The data is massive (Gigabytes or Terabytes) and exceeds RAM, as HDF5 allows reading/writing slices of data from disk without loading the whole file.\n",
    "* **Interoperability:** The data needs to be read by other languages (C++, MATLAB, R), as Pickle is Python-specific.\n",
    "* **Security:** Loading files from untrusted sources, as `pickle` is insecure and can execute arbitrary code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487031b",
   "metadata": {},
   "source": [
    "**Checkpointing and Serialization**\n",
    "\n",
    "To ensure experiment reproducibility and scalability, we implemented a serialization pipeline using Python's `pickle` protocol. We encapsulate the model state—including trainable parameters, optimizer states, and hyperparameter metadata—into a dictionary structure.\n",
    "\n",
    "These artifacts are persisted to the `models/` directory using the naming convention `model_<track>_<nqubits>.pkl`. This approach allows for rapid saving and loading of experiments without the overhead of external database dependencies, which is suitable for the current scale of data (under 1GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f991a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization helpers (implement with pickle)\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    \"\"\"Serialize `obj` to `path` using pickle.\"\"\"\n",
    "    target_path = Path(path)\n",
    "    # Ensure the parent directory (e.g., 'models/') exists\n",
    "    target_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(target_path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    print(f\"Artifact saved to: {target_path}\")\n",
    "\n",
    "def load_pickle(path):\n",
    "    \"\"\"Deserialize an object from `path`.\"\"\"\n",
    "    target_path = Path(path)\n",
    "    if not target_path.exists():\n",
    "        raise FileNotFoundError(f\"No file found at {target_path}\")\n",
    "        \n",
    "    with open(target_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def demonstrate_serialization_roundtrip():\n",
    "    \"\"\"Create a quick round-trip save/load test and return the restored object.\"\"\"\n",
    "    # 1. Create dummy data mimicking a tomography model state\n",
    "    test_data = {\n",
    "        \"n_qubits\": 3,\n",
    "        \"track\": \"density_matrix\",\n",
    "        \"weights\": [0.12, 0.55, 0.9],\n",
    "        \"metadata\": \"Test run\"\n",
    "    }\n",
    "    \n",
    "    # 2. Define path according to naming convention: model_<track>_<nqubits>.pkl\n",
    "    # Placing it in models/ as requested\n",
    "    test_path = \"models/model_test_3.pkl\"\n",
    "    \n",
    "    # 3. Perform round-trip\n",
    "    print(\"--- Starting Serialization Test ---\")\n",
    "    save_pickle(test_data, test_path)\n",
    "    restored_data = load_pickle(test_path)\n",
    "    \n",
    "    # 4. Validation\n",
    "    assert test_data == restored_data, \"Mismatch between original and restored data!\"\n",
    "    print(\"--- Success: Data restored correctly ---\")\n",
    "    \n",
    "    return restored_data\n",
    "\n",
    "# Execute the test\n",
    "demonstrate_serialization_roundtrip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452807df",
   "metadata": {},
   "source": [
    "## Task 2 · Extendable n-qubit surrogate\n",
    "Create a model class that accepts `n_qubits` and optional settings like layer count, hidden size, or noise switches. The scaffold below still uses a simple complex vector. Replace the `statevector` logic with your own design but keep the public methods (`save`, `load`, `fidelity_with`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: scalable n-qubit tomography surrogate\n",
    "import numpy as np\n",
    "\n",
    "class QuantumModel:\n",
    "    def __init__(self, n_qubits, n_layers=1, params=None, seed=None):\n",
    "        \"\"\"TODO: Initialize model attributes, RNG, and parameter vector.\"\"\"\n",
    "        raise NotImplementedError(\"Populate constructor with initialization logic.\")\n",
    "\n",
    "    def statevector(self):\n",
    "        \"\"\"TODO: Return a normalized complex statevector built from model parameters.\"\"\"\n",
    "        raise NotImplementedError(\"Derive the statevector for the configured model.\")\n",
    "\n",
    "    def fidelity_with(self, target_state):\n",
    "        \"\"\"TODO: Compute fidelity between the model statevector and `target_state`.\"\"\"\n",
    "        raise NotImplementedError(\"Implement fidelity calculation for pure states.\")\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"TODO: Persist the trained model using `save_pickle`.\"\"\"\n",
    "        raise NotImplementedError(\"Call save_pickle with appropriate metadata.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"TODO: Restore a saved model instance using `load_pickle`.\"\"\"\n",
    "        raise NotImplementedError(\"Call load_pickle and return the restored model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33982ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalable n-qubit tomography surrogate\n",
    "import numpy as np\n",
    "\n",
    "class QuantumModel:\n",
    "    def __init__(self, n_qubits, n_layers=1, params=None, seed=None):\n",
    "        \"\"\"Initialize model attributes, RNG, and parameter vector.\"\"\"\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers  # Stored for metadata/ablation studies\n",
    "        self.dim = 2**n_qubits\n",
    "        \n",
    "        # Set up Random Number Generator\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        \n",
    "        # Initialize parameters (Complex Statevector Amplitudes)\n",
    "        if params is None:\n",
    "            # Initialize random complex vector (real + imaginary parts)\n",
    "            real_part = self.rng.normal(0, 1, self.dim)\n",
    "            imag_part = self.rng.normal(0, 1, self.dim)\n",
    "            self.params = real_part + 1j * imag_part\n",
    "        else:\n",
    "            self.params = np.array(params, dtype=np.complex128)\n",
    "            \n",
    "        # Ensure initial normalization\n",
    "        norm = np.linalg.norm(self.params)\n",
    "        if norm > 1e-9:\n",
    "            self.params /= norm\n",
    "\n",
    "    def statevector(self):\n",
    "        \"\"\"Return a normalized complex statevector built from model parameters.\"\"\"\n",
    "        # Always re-normalize on the fly to ensure valid quantum state\n",
    "        norm = np.linalg.norm(self.params)\n",
    "        if norm < 1e-9:\n",
    "            # Fallback for zero-vector edge case\n",
    "            psi = np.zeros_like(self.params)\n",
    "            psi[0] = 1.0\n",
    "            return psi\n",
    "        return self.params / norm\n",
    "\n",
    "    def fidelity_with(self, target_state):\n",
    "        \"\"\"Compute fidelity between the model statevector and `target_state`.\"\"\"\n",
    "        # Get current normalized model state\n",
    "        psi = self.statevector()\n",
    "        \n",
    "        # Ensure target is a numpy array\n",
    "        phi = np.array(target_state)\n",
    "        \n",
    "        # Compute overlap: |<psi|phi>|^2\n",
    "        overlap = np.vdot(psi, phi)  # vdot handles complex conjugation correctly\n",
    "        fidelity = np.abs(overlap)**2\n",
    "        return float(fidelity)\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"Persist the trained model using `save_pickle`.\"\"\"\n",
    "        # Pack everything needed to reconstruct the model\n",
    "        checkpoint = {\n",
    "            \"n_qubits\": self.n_qubits,\n",
    "            \"n_layers\": self.n_layers,\n",
    "            \"params\": self.params,\n",
    "            # We don't necessarily save RNG state unless exact training resumption is required\n",
    "        }\n",
    "        save_pickle(checkpoint, path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"Restore a saved model instance using `load_pickle`.\"\"\"\n",
    "        data = load_pickle(path)\n",
    "        \n",
    "        # Re-instantiate the class with loaded data\n",
    "        model = QuantumModel(\n",
    "            n_qubits=data[\"n_qubits\"],\n",
    "            n_layers=data.get(\"n_layers\", 1), # Default to 1 if missing\n",
    "            params=data[\"params\"]\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeac1be",
   "metadata": {},
   "source": [
    "## Task 3 · Scalability study\n",
    "Check how fidelity and runtime change when you add more qubits. Track both averages and spread across random seeds. Discuss how expressibility, noise, or optimisation choices slow you down as `n` grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed8aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: scalability experiments\n",
    "import csv\n",
    "import time\n",
    "\n",
    "def random_pure_state(dim, rng):\n",
    "    \"\"\"TODO: Sample a normalized random complex state of size `dim`.\"\"\"\n",
    "    raise NotImplementedError(\"Implement random state sampling.\")\n",
    "\n",
    "def scalability_experiment(qubit_list, trials=10, n_layers=1, seed=0):\n",
    "    \"\"\"TODO: Benchmark fidelity and runtime for each entry in `qubit_list`.\"\"\"\n",
    "    raise NotImplementedError(\"Implement experiment loop and return a summary list of dicts.\")\n",
    "\n",
    "def save_scalability_summary(summary, path='scalability_results.csv'):\n",
    "    \"\"\"TODO: Persist the summary data to CSV for downstream plotting.\"\"\"\n",
    "    raise NotImplementedError(\"Write the summary rows to `path` using csv.DictWriter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c971c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: scalability experiments\n",
    "import csv\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Ensure you have run the cell defining QuantumModel before this!\n",
    "\n",
    "def random_pure_state(dim, rng):\n",
    "    \"\"\"Sample a normalized random complex state of size `dim`.\"\"\"\n",
    "    # Sample real and imaginary parts from a Gaussian distribution\n",
    "    real_part = rng.normal(0, 1, dim)\n",
    "    imag_part = rng.normal(0, 1, dim)\n",
    "    psi = real_part + 1j * imag_part\n",
    "    # Normalize to create a valid quantum state\n",
    "    psi /= np.linalg.norm(psi)\n",
    "    return psi\n",
    "\n",
    "def scalability_experiment(qubit_list, trials=10, n_layers=1, seed=0):\n",
    "    \"\"\"Benchmark fidelity and runtime for each entry in `qubit_list`.\"\"\"\n",
    "    results = []\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    print(f\"Starting scalability run for qubits: {qubit_list} with {trials} trials each.\")\n",
    "\n",
    "    for n_qubits in qubit_list:\n",
    "        dim = 2**n_qubits\n",
    "        \n",
    "        for t in range(trials):\n",
    "            # 1. Generate a random target state (simulating 'ground truth')\n",
    "            target_state = random_pure_state(dim, rng)\n",
    "            \n",
    "            # Start timer\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # 2. Initialize the model\n",
    "            # Note: In a real tomography run, you would also include your \n",
    "            # optimization/training loop here (e.g., optimizer.step()).\n",
    "            # For this baseline, we measure initialization + forward pass overhead.\n",
    "            model = QuantumModel(n_qubits, n_layers=n_layers, seed=rng.integers(10000))\n",
    "            \n",
    "            # 3. Measure fidelity\n",
    "            # (Without training, this measures the fidelity of a random initialization)\n",
    "            fid = model.fidelity_with(target_state)\n",
    "            \n",
    "            # Stop timer\n",
    "            dt = time.time() - t0\n",
    "            \n",
    "            # Log data\n",
    "            results.append({\n",
    "                \"n_qubits\": n_qubits,\n",
    "                \"trial\": t,\n",
    "                \"fidelity\": fid,\n",
    "                \"runtime\": dt,\n",
    "                \"n_layers\": n_layers\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_scalability_summary(summary, path='scalability_results.csv'):\n",
    "    \"\"\"Persist the summary data to CSV for downstream plotting.\"\"\"\n",
    "    if not summary:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    # Automatically detect columns from the first result\n",
    "    fieldnames = summary[0].keys()\n",
    "    \n",
    "    with open(path, 'w', newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(summary)\n",
    "    \n",
    "    print(f\"Scalability results saved to {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e1c70",
   "metadata": {},
   "source": [
    "As (number of qubits) grows, the \"curse of dimensionality\" affects the pipeline in three ways:\n",
    "\n",
    "1. **Expressibility (The Parameter Problem):**\n",
    "For a statevector model, the number of parameters grows as .\n",
    "* *Impact:* Doubling the qubits doesn't just double the memory; it squares the state space complexity. Initialization and normalization (calculating `np.linalg.norm`) become exponentially slower.\n",
    "\n",
    "\n",
    "2. **Noise Accumulation:**\n",
    "In real hardware (or noisy simulations), fidelity drops exponentially with depth ().\n",
    "* *Impact:* As  increases, you need more \"shots\" (measurements) to distinguish the signal from the noise, meaning the *optimization loop* (not shown in the simplified code above) would need to run longer to converge.\n",
    "\n",
    "\n",
    "3. **Optimization Landscape:**\n",
    "With higher , the \"Barren Plateau\" problem becomes significant. The gradients variance vanishes exponentially, meaning the optimizer gets stuck on flat surfaces and takes significantly more iterations to find a good direction, increasing runtime drastically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18d8a7",
   "metadata": {},
   "source": [
    "## Task 4 · Visualise scalability metrics\n",
    "Plot mean fidelity with error bars and runtime for each qubit count. Include at least one figure in your submission and describe where scaling starts to hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda4242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: scalability plotting helper\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scalability(csv_path='scalability_results.csv'):\n",
    "    \"\"\"TODO: Load the CSV produced by `save_scalability_summary` and render fidelity/runtime plots.\"\"\"\n",
    "    raise NotImplementedError(\"Create subplot visualizations with error bars and runtime curve.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698e98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: scalability plotting helper\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_scalability(csv_path='scalability_results.csv'):\n",
    "    \"\"\"Load the CSV produced by `save_scalability_summary` and render fidelity/runtime plots.\"\"\"\n",
    "    \n",
    "    # 1. Load Data\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {csv_path} not found. Please run the scalability experiment (Task 3) first.\")\n",
    "        return\n",
    "\n",
    "    # 2. Aggregate Data\n",
    "    # We group by 'n_qubits' to calculate the mean and standard deviation (for error bars)\n",
    "    # for both fidelity and runtime.\n",
    "    summary = df.groupby('n_qubits')[['fidelity', 'runtime']].agg(['mean', 'std'])\n",
    "    \n",
    "    # 3. Create Visualizations\n",
    "    # We use 1 row, 2 columns of subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # -- Plot 1: Fidelity vs Qubits --\n",
    "    # fmt='-o' connects points with a line and marks them with circles\n",
    "    ax1.errorbar(\n",
    "        summary.index, \n",
    "        summary['fidelity']['mean'], \n",
    "        yerr=summary['fidelity']['std'], \n",
    "        fmt='-o', \n",
    "        capsize=5, \n",
    "        color='#1f77b4', \n",
    "        label='Mean Fidelity'\n",
    "    )\n",
    "    ax1.set_title('Reconstruction Fidelity vs Qubit Count')\n",
    "    ax1.set_xlabel('Number of Qubits (n)')\n",
    "    ax1.set_ylabel('Fidelity')\n",
    "    ax1.set_ylim(-0.05, 1.05) # Keep view within valid fidelity range\n",
    "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # -- Plot 2: Runtime vs Qubits --\n",
    "    ax2.errorbar(\n",
    "        summary.index, \n",
    "        summary['runtime']['mean'], \n",
    "        yerr=summary['runtime']['std'], \n",
    "        fmt='-s', \n",
    "        capsize=5, \n",
    "        color='#d62728', \n",
    "        label='Mean Runtime'\n",
    "    )\n",
    "    ax2.set_title('Computational Runtime vs Qubit Count')\n",
    "    ax2.set_xlabel('Number of Qubits (n)')\n",
    "    ax2.set_ylabel('Runtime (seconds)')\n",
    "    ax2.set_yscale('log') # Log scale is crucial for visualizing exponential scaling\n",
    "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # 4. Save Plot\n",
    "    output_filename = 'scalability_metrics.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_filename)\n",
    "    print(f\"Scalability visualization saved to: {output_filename}\")\n",
    "\n",
    "# Example usage (ensure you have the CSV from Task 3):\n",
    "plot_scalability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b746070",
   "metadata": {},
   "source": [
    "**The Exponential Wall**\n",
    "\n",
    "Scaling \"hurts\" when the computational resources required grow faster than the value gained. In this statevector simulation, you will observe two distinct scaling cliffs:\n",
    "\n",
    "1. **Runtime (The  Cliff):**\n",
    "* **Observation:** On the log-scale runtime plot (Right), you should see a straight line ascending. This indicates exponential growth.\n",
    "* **The Limit:** For a typical Python/NumPy implementation, operations usually remain instant () up to ****. Beyond ****, the time doubles with every added qubit. By ****, a single matrix multiplication or state generation can take seconds to minutes, making iterative optimization (which runs thousands of times) unfeasible on a laptop.\n",
    "\n",
    "\n",
    "2. **Fidelity (The Concentration of Measure):**\n",
    "* **Observation:** The fidelity plot (Left) will likely show a sharp decline or remain near zero (if untrained).\n",
    "* **The Limit:** As  grows, the Hilbert space volume explodes (). If you initialize a model randomly, the probability of it having any significant overlap with a random target state drops exponentially (). Even with training, optimization landscapes become flatter (Barren Plateaus), making it exponentially harder to \"find\" the solution as  increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c83ca",
   "metadata": {},
   "source": [
    "## Task 5 · Ablation studies\n",
    "Test how design choices (depth, parameter style, noise models) affect fidelity. Extend the scaffold with extra factors that fit your track, such as quantisation level or spike encoding.\n",
    "\n",
    "**Deliverables**\n",
    "- Write an ablation plan with hypotheses, references, and metrics before you code.\n",
    "- Extend the code templates with the architecture or training variants you need.\n",
    "- Record mean fidelity, variance, runtime and build tables or plots for your report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e148d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template: ablation study scaffold\n",
    "def ablation_layers(n_qubits=3, layer_list=None, trials=30, seed=1):\n",
    "    \"\"\"TODO: Vary architecture depth and record aggregate fidelity statistics.\"\"\"\n",
    "    raise NotImplementedError(\"Implement ablation loop returning a list of summary dicts.\")\n",
    "\n",
    "def summarize_ablation_results(results):\n",
    "    \"\"\"TODO: Format the ablation output for reporting (tables/plots/logs).\"\"\"\n",
    "    raise NotImplementedError(\"Aggregate and present ablation metrics for documentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a91c040",
   "metadata": {},
   "source": [
    "## Task 6 · Reporting and submission\n",
    "Write your findings in `docs/` and commit the `.pkl` checkpoints. Reflect on scaling limits, ablation notes, and next moves such as classical shadows or hardware tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ddac78",
   "metadata": {},
   "source": [
    "### Submission checklist\n",
    "- `.pkl` checkpoints inside `models/` with a quick README note on how to load them.\n",
    "- Notebook outputs that show save/load, scalability numbers, and ablation tables.\n",
    "- Plots that highlight fidelity vs qubits and runtime trends.\n",
    "- A written summary covering method, limits, and future experiments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
